{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvBmaFFfokhN/ACcN1meR8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klbgFS0nupCA",
        "outputId": "9ad7c8cb-81f3-4924-b878-02200dab4ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\") #nltk'in wordnet sözlük veritabanını indirmemizi sağlar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer #stemming için fonksiyon\n",
        "\n",
        "stemmer=PorterStemmer() #porter stemmer nesnesini oluşturur\n",
        "\n",
        "words=[\"running\",\"runner\",\"ran\",\"runs\",\"better\",\"go\",\"went\",\"languages\"]\n",
        "\n",
        "#kelimelerin stem'lerini buluyoruz, bunu yaparken de portner stemmerin stem() fonks. kullanıyoruz\n",
        "stems = [stemmer.stem(w) for w in words]  #stemmer.stem(w) → her kelimenin kökünü çıkar (stemming)\n",
        "print(f\"Stem: {stems}\")"
      ],
      "metadata": {
        "id": "T0exaF0mwKmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2db1fd-24bc-444c-ff99-f84d1a2f5649"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem: ['run', 'runner', 'ran', 'run', 'better', 'go', 'went', 'languag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words=[\"running\",\"runner\",\"ran\",\"runs\",\"better\",\"go\",\"went\",\"languages\"]\n",
        "lemmas=[lemmatizer.lemmatize(w, pos=\"v\")for w in words]   #pos=\"v\" dizide daha çok fiil kelimeler olduğu için daha iyi bir sonuç çıkarsın diye verb \"v\" yazdık languages fiil olmadığı için\n",
        "lemmas_noun=[lemmatizer.lemmatize(w, pos=\"n\")for w in words]   #pos=\"n\" isimlerde daha iyi bir sonuç çıkarsın diye verb \"n\" yazdık languges doğru çıktı\n",
        "\n",
        "print(f\"Lemmas: {lemmas}\")\n",
        "print(f\"Lemmas_noun: {lemmas_noun}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn1RMfhkNayI",
        "outputId": "4ed8fcdd-6f58-4fb2-cc0c-8f3252a742f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmas: ['run', 'runner', 'run', 'run', 'better', 'go', 'go', 'languages']\n",
            "Lemmas_noun: ['running', 'runner', 'ran', 'run', 'better', 'go', 'went', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zeyrek #python için zeyrek kütüphanesini indir ve kur türkçe için (zeyrek tokenization yapmıyor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At6lTUeFPG91",
        "outputId": "a245c66b-0182-45e7-d801-dec806d2a1f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zeyrek\n",
            "  Downloading zeyrek-0.1.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: nltk>=3 in /usr/local/lib/python3.12/dist-packages (from zeyrek) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3->zeyrek) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3->zeyrek) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3->zeyrek) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3->zeyrek) (4.67.1)\n",
            "Downloading zeyrek-0.1.3-py2.py3-none-any.whl (930 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/931.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.0/931.0 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zeyrek\n",
            "Successfully installed zeyrek-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk  #zeyrek tokenization yapmıyor bu yüzden nltk'den yardım alıyoruz\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoONaOH-Pixt",
        "outputId": "46362ba0-ab90-4043-8a01-94dcaeaeca57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cumleler=[\"Bugün çok koşuyorum\",\"koşucu\",\"koşmak\",\"koştum\",\"kitaplar\",\"evlerde\",\"güzellik\"]"
      ],
      "metadata": {
        "id": "J7FbQTzuP1Gs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zeyrek\n",
        "analyzer=zeyrek.MorphAnalyzer() #MorphAnalyzer() sınıfınfan bir nesne oluşturuyoruz"
      ],
      "metadata": {
        "id": "rz1lRcOfQGet"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zeyrek_words=[]\n",
        "for cumle in cumleler:   #cümleler listesindeki her öğreyi sırasıyla alıp köklerini buluyoruz\n",
        "  zeyrek_words.append(analyzer.lemmatize(cumle))\n",
        "print(zeyrek_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uPjNxfyQWeF",
        "outputId": "e0bd0651-b27d-46a6-855c-b0d3d52af557"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(bugün_Adv)(-)(bugün:advRoot_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(bugün_Noun_Time)(-)(bugün:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + uyor:vProgYor_S + um:vA1sg_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + ucu:vAgt_S + adjAfterVerb_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + ucu:vAgt_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(koşu_Noun)(-)(koşu:noun_S + a3sg_S + pnon_S + nom_ST + cu:agt_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + mak:vInf1_S + nounInf1Root_S + a3sgInf1_S + pnonInf1_S + nom_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + tu:vPast_S + m:vA1sg_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(kitap_Noun)(-)(kitap:noun_S + lar:a3pl_S + pnon_S + nom_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(Kitap_Noun_Prop)(-)(kitap:nounProper_S + lar:a3pl_S + pnon_S + nom_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(kitap_Noun)(-)(kitap:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + lar:nA3pl_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(Kitap_Noun_Prop)(-)(kitap:nounProper_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + lar:nA3pl_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(ev_Noun)(-)(ev:noun_S + ler:a3pl_S + pnon_S + de:loc_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(güzel_Adj)(-)(güzel:adjectiveRoot_ST + lik:ness_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
            "WARNING:zeyrek.rulebasedanalyzer:APPENDING RESULT: <(güzel_Noun)(-)(güzel:noun_S + a3sg_S + pnon_S + nom_ST + lik:ness_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('Bugün', ['bugün']), ('çok', ['çok']), ('koşuyorum', ['koşmak'])], [('koşucu', ['koşmak', 'koşu'])], [('koşmak', ['koşmak'])], [('koştum', ['koşmak'])], [('kitaplar', ['Kitap', 'kitap'])], [('evlerde', ['ev'])], [('güzellik', ['güzel'])]]\n"
          ]
        }
      ]
    }
  ]
}